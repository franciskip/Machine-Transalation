{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MT_EN_to_SW_MachineTranslation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti6fADMygSV9"
      },
      "source": [
        "## MACHINE TRANSLATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_smPZSuirdW"
      },
      "source": [
        "# kipkoh=[]\n",
        "# while(1):\n",
        "#   kipkoh.append('1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es5pHqFdwtXr"
      },
      "source": [
        "import os\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model\n",
        "import string\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAGGWZ1XmKoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d589920-d510-49ef-b110-2357c513b826"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7DTJ_SmmccX",
        "outputId": "7f495011-5f47-447b-9a02-61d0fed3310b"
      },
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.5.30)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2021.10.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 11.1 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16367 sha256=9937ac12ed65e915711258344955a24b3b0b9cdb0dccf2670ae458a7da3fae13\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/be/fe/93a6a40ffe386e16089e44dad9018ebab9dc4cb9eb7eab65ae\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.10.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l87BkOVAxyXe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8T14xNYd9Jf"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "\n",
        "import googletrans\n",
        "from googletrans import Translator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3iVrUfFSTBA",
        "outputId": "b1b916c0-8fee-407c-d60b-6af3261ba861"
      },
      "source": [
        "! ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-RhWICOD7v6"
      },
      "source": [
        "### Translate English articles to Swahili articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "hM1KIZQNq91X",
        "outputId": "e3aec65a-9ddb-4424-f82f-36b4b7536c46"
      },
      "source": [
        "pd.set_option('max_colwidth', 300)\n",
        " \n",
        "# how to get the supported language and their corresponing code\n",
        "lang_df = pd.DataFrame.from_dict(googletrans.LANGUAGES,  orient='index', columns=['Language'])\n",
        "print(len(googletrans.LANGUAGES))\n",
        "lang_df.head(3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>af</th>\n",
              "      <td>afrikaans</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sq</th>\n",
              "      <td>albanian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>am</th>\n",
              "      <td>amharic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Language\n",
              "af  afrikaans\n",
              "sq   albanian\n",
              "am    amharic"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "-b-nWdVTrdHK",
        "outputId": "35c3f53e-15a8-4e7f-8194-2c8a29fefae7"
      },
      "source": [
        "# find the code for english and swahili\n",
        "lang_df[lang_df.Language.isin(['english', 'swahili'])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>en</th>\n",
              "      <td>english</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sw</th>\n",
              "      <td>swahili</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Language\n",
              "en  english\n",
              "sw  swahili"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zCa2JdvlcW0",
        "outputId": "9cea476f-9e59-4956-f66f-2dea52df4a3a"
      },
      "source": [
        "# os.environ[\"gdrive_path\"]='/content/drive/MyDrive/colab/MT_TRANSALATOR/'\n",
        "!ls /content/drive/MyDrive/MT_TRANSALATOR/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/MyDrive/MT_TRANSALATOR/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "m4hJx-VQol_8",
        "outputId": "8e2ba67c-67f2-4f27-e17e-2ce9d03badbb"
      },
      "source": [
        "filename = '/content/drive/MyDrive/colab/MT_TRANSALATOR/english.en'\n",
        "file = open(filename, mode='r') # 'r' is to read\n",
        "en = pd.DataFrame(file)\n",
        "en = en.rename(columns={0:'EnglishText'})\n",
        "en.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EnglishText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Article 2\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Netflix advertisement for the movie Santana, to be premiered on 28 August.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Screengrab.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Image text: I want Dias Santana and his brother dead\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From August 28, an Angolan production will appear for the first time in Netflix's catalogue.\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                      EnglishText\n",
              "0                                                                                     Article 2\\n\n",
              "1                    Netflix advertisement for the movie Santana, to be premiered on 28 August.\\n\n",
              "2                                                                                   Screengrab.\\n\n",
              "3                                          Image text: I want Dias Santana and his brother dead\\n\n",
              "4  From August 28, an Angolan production will appear for the first time in Netflix's catalogue.\\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "3HEQwTc4o6AV",
        "outputId": "2666bd26-7974-41ff-a9ca-ef52f737ac66"
      },
      "source": [
        "filename1 = '/content/drive/MyDrive/colab/MT_TRANSALATOR/swahili.sw'\n",
        "file1 = open(filename1, mode='r') # 'r' is to read\n",
        "sw = pd.DataFrame(file1)\n",
        "sw = sw.rename(columns={0:\"SwahiliText\"})\n",
        "sw.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SwahiliText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Makala 2\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tangazo la Netflix kwa ajili ya filamu ya Santana, itakayokwenda hewani mnamo Agosti 28.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Picha imepigwa kwenye filamu.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Maneno yanayoonekana yanasema: Ninataka Dias Santana na kaka yake wauawe\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kuanzia Agosti 28, filamu iliyozalishwa Angola itaonekana kwa mara ya kwanza katika orodha ya Netflix.\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                SwahiliText\n",
              "0                                                                                                Makala 2\\n\n",
              "1                Tangazo la Netflix kwa ajili ya filamu ya Santana, itakayokwenda hewani mnamo Agosti 28.\\n\n",
              "2                                                                           Picha imepigwa kwenye filamu.\\n\n",
              "3                                Maneno yanayoonekana yanasema: Ninataka Dias Santana na kaka yake wauawe\\n\n",
              "4  Kuanzia Agosti 28, filamu iliyozalishwa Angola itaonekana kwa mara ya kwanza katika orodha ya Netflix.\\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "bJFqsh2Uouv1",
        "outputId": "7c9ef6de-d523-44e0-ac92-acd305013e41"
      },
      "source": [
        "en['SwahiliText']=sw.SwahiliText\n",
        "en.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EnglishText</th>\n",
              "      <th>SwahiliText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Article 2\\n</td>\n",
              "      <td>Makala 2\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Netflix advertisement for the movie Santana, to be premiered on 28 August.\\n</td>\n",
              "      <td>Tangazo la Netflix kwa ajili ya filamu ya Santana, itakayokwenda hewani mnamo Agosti 28.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Screengrab.\\n</td>\n",
              "      <td>Picha imepigwa kwenye filamu.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Image text: I want Dias Santana and his brother dead\\n</td>\n",
              "      <td>Maneno yanayoonekana yanasema: Ninataka Dias Santana na kaka yake wauawe\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From August 28, an Angolan production will appear for the first time in Netflix's catalogue.\\n</td>\n",
              "      <td>Kuanzia Agosti 28, filamu iliyozalishwa Angola itaonekana kwa mara ya kwanza katika orodha ya Netflix.\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                      EnglishText                                                                                               SwahiliText\n",
              "0                                                                                     Article 2\\n                                                                                                Makala 2\\n\n",
              "1                    Netflix advertisement for the movie Santana, to be premiered on 28 August.\\n                Tangazo la Netflix kwa ajili ya filamu ya Santana, itakayokwenda hewani mnamo Agosti 28.\\n\n",
              "2                                                                                   Screengrab.\\n                                                                           Picha imepigwa kwenye filamu.\\n\n",
              "3                                          Image text: I want Dias Santana and his brother dead\\n                                Maneno yanayoonekana yanasema: Ninataka Dias Santana na kaka yake wauawe\\n\n",
              "4  From August 28, an Angolan production will appear for the first time in Netflix's catalogue.\\n  Kuanzia Agosti 28, filamu iliyozalishwa Angola itaonekana kwa mara ya kwanza katika orodha ya Netflix.\\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyUXkhodo5kn",
        "outputId": "f083ab40-300c-43bd-d104-9c0d3a8b188d"
      },
      "source": [
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opustools-pkg\n",
            "  Downloading opustools_pkg-0.0.52-py3-none-any.whl (80 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 20 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 80 kB 4.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBhvPOprqBwo"
      },
      "source": [
        "en_df=en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCXVXVcwtVrZ",
        "outputId": "9d836255-af41-4ce5-9cbb-7c12f8f57221"
      },
      "source": [
        "en_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3815, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "J1bHn64juMmW",
        "outputId": "e42442ac-2a47-40d7-8377-2598b3548628"
      },
      "source": [
        "en_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EnglishText</th>\n",
              "      <th>SwahiliText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Article 2\\n</td>\n",
              "      <td>Makala 2\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Netflix advertisement for the movie Santana, to be premiered on 28 August.\\n</td>\n",
              "      <td>Tangazo la Netflix kwa ajili ya filamu ya Santana, itakayokwenda hewani mnamo Agosti 28.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Screengrab.\\n</td>\n",
              "      <td>Picha imepigwa kwenye filamu.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Image text: I want Dias Santana and his brother dead\\n</td>\n",
              "      <td>Maneno yanayoonekana yanasema: Ninataka Dias Santana na kaka yake wauawe\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From August 28, an Angolan production will appear for the first time in Netflix's catalogue.\\n</td>\n",
              "      <td>Kuanzia Agosti 28, filamu iliyozalishwa Angola itaonekana kwa mara ya kwanza katika orodha ya Netflix.\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                      EnglishText                                                                                               SwahiliText\n",
              "0                                                                                     Article 2\\n                                                                                                Makala 2\\n\n",
              "1                    Netflix advertisement for the movie Santana, to be premiered on 28 August.\\n                Tangazo la Netflix kwa ajili ya filamu ya Santana, itakayokwenda hewani mnamo Agosti 28.\\n\n",
              "2                                                                                   Screengrab.\\n                                                                           Picha imepigwa kwenye filamu.\\n\n",
              "3                                          Image text: I want Dias Santana and his brother dead\\n                                Maneno yanayoonekana yanasema: Ninataka Dias Santana na kaka yake wauawe\\n\n",
              "4  From August 28, an Angolan production will appear for the first time in Netflix's catalogue.\\n  Kuanzia Agosti 28, filamu iliyozalishwa Angola itaonekana kwa mara ya kwanza katika orodha ya Netflix.\\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_9K4UhCuYqe",
        "outputId": "ce8b88ee-c96b-429d-8582-5d78049cee40"
      },
      "source": [
        "en_df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['EnglishText', 'SwahiliText'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_1FxM01uRMx"
      },
      "source": [
        "en_df.rename(columns={'EnglishText':'source_sentence','SwahiliText':'target_sentence'},inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "YZXQ12ezYHfh",
        "outputId": "b24bc2a8-42d3-426a-97b7-6b14fbfa521c"
      },
      "source": [
        "df1=pd.read_csv('/content/drive/MyDrive/colab/MT_TRANSALATOR/data.csv')\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>“ Look !</td>\n",
              "      <td>“ Tazama !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I Am Making All Things New ”</td>\n",
              "      <td>Mimi Ninafanya Vitu Vyote Kuwa Vipya ”</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The above is a promise from God that has been adopted as the title of an inspiring brochure .</td>\n",
              "      <td>Iliyopo juu ni ahadi itokayo kwa Mungu ambayo imechaguliwa iwe kichwa cha broshua yenye kuvuvia tumaini .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>( Revelation 21 : 5 ) What does that promise mean for us ?</td>\n",
              "      <td>( Ufunuo 21 : 5 , NW ) Ahadi hiyo yamaanisha nini kwetu ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How will it be fulfilled , and when ?</td>\n",
              "      <td>Itatimizwa jinsi gani , na wakati gani ?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                 source_sentence                                                                                            target_sentence\n",
              "0                                                                                       “ Look !                                                                                                 “ Tazama !\n",
              "1                                                                   I Am Making All Things New ”                                                                     Mimi Ninafanya Vitu Vyote Kuwa Vipya ”\n",
              "2  The above is a promise from God that has been adopted as the title of an inspiring brochure .  Iliyopo juu ni ahadi itokayo kwa Mungu ambayo imechaguliwa iwe kichwa cha broshua yenye kuvuvia tumaini .\n",
              "3                                     ( Revelation 21 : 5 ) What does that promise mean for us ?                                                  ( Ufunuo 21 : 5 , NW ) Ahadi hiyo yamaanisha nini kwetu ?\n",
              "4                                                          How will it be fulfilled , and when ?                                                                   Itatimizwa jinsi gani , na wakati gani ?"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9kxewM4xYeO"
      },
      "source": [
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"sw\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!sudo mkdir -p \"/content/drive/MyDrive/colab/MLT/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/MyDrive/colab/MLT/%s-%s-%s\" % (source_language, target_language, tag)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ3cvtbmUjJM",
        "outputId": "d696b2f2-6c98-4309-e5ce-e1a8ea57456f"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colab/MLT/en-sw-baseline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXXNeN9CV5Up",
        "outputId": "acc9b406-3361-49d8-8709-d5601386ca9b"
      },
      "source": [
        "!ls /content/drive/MyDrive/colab/MLT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en-sw-baseline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwU7PoedU0N_",
        "outputId": "1991215b-c5b7-4475-904d-3af32bad2669"
      },
      "source": [
        "\n",
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opustools-pkg in /usr/local/lib/python3.7/dist-packages (0.0.52)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQS1o5W_U3jg"
      },
      "source": [
        "# # Downloading our corpus\n",
        "# ! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
        "\n",
        "# # extract the corpus file\n",
        "# ! gunzip JW300_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klx2l193Z_xR"
      },
      "source": [
        "d2=df1.sample(n=60000,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5cvYR3LxdEI"
      },
      "source": [
        "df=df1.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3it0Us1Z9_O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbU_8L9g1xYG",
        "outputId": "45da169c-22b3-427c-819f-4337ff90fc5f"
      },
      "source": [
        "# Download the global test set.\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
        "  \n",
        "# And the specific test set for this language pair.\n",
        "os.environ[\"trg\"] = target_language \n",
        "os.environ[\"src\"] = source_language \n",
        "\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
        "! mv test.en-$trg.en test.en\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
        "! mv test.en-$trg.$trg test.$trg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-22 07:00:21--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277791 (271K) [text/plain]\n",
            "Saving to: ‘test.en-any.en’\n",
            "\n",
            "test.en-any.en      100%[===================>] 271.28K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-10-22 07:00:22 (8.69 MB/s) - ‘test.en-any.en’ saved [277791/277791]\n",
            "\n",
            "--2021-10-22 07:00:22--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-sw.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 206336 (202K) [text/plain]\n",
            "Saving to: ‘test.en-sw.en’\n",
            "\n",
            "test.en-sw.en       100%[===================>] 201.50K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-10-22 07:00:22 (8.39 MB/s) - ‘test.en-sw.en’ saved [206336/206336]\n",
            "\n",
            "--2021-10-22 07:00:22--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-sw.sw\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 214836 (210K) [text/plain]\n",
            "Saving to: ‘test.en-sw.sw’\n",
            "\n",
            "test.en-sw.sw       100%[===================>] 209.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-10-22 07:00:22 (8.54 MB/s) - ‘test.en-sw.sw’ saved [214836/214836]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycwskg__2nVu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlQcwnIJ2AA-",
        "outputId": "ad27cc29-7948-4e2d-d68c-7c45009bb696"
      },
      "source": [
        "# Read the test data to filter from train and dev splits.\n",
        "# Store english portion in set for quick filtering checks.\n",
        "en_test_sents = set()\n",
        "filter_test_sents = \"test.en-any.en\"\n",
        "j = 0\n",
        "with open(filter_test_sents) as f:\n",
        "  for line in f:\n",
        "    en_test_sents.add(line.strip())\n",
        "    j += 1\n",
        "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3571 global test sentences to filter from the training/dev data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njzdqOUZ2V0Z"
      },
      "source": [
        "# filename1 = 'test.en-any.en'\n",
        "# file1 = open(filename1, mode='r') # 'r' is to read\n",
        "# test1 = pd.DataFrame(file1)\n",
        "# test1=test1.sample(n=500,random_state=42)\n",
        "# test1.to_csv('test.en-any.en', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h49us1Xf5_-U"
      },
      "source": [
        "# filename1 = 'test.en-any.en'\n",
        "# file1 = open(filename1, mode='r') # 'r' is to read\n",
        "# test1 = pd.DataFrame(file1)\n",
        "# test1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lipMo5lDxo7I"
      },
      "source": [
        "####Pre-processing and export\n",
        "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
        "\n",
        "In addition we will split our data into dev/test/train and export to the filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tIerMUdxq-K",
        "outputId": "ce4fbcf2-a5ea-452f-9009-b25110cc450d"
      },
      "source": [
        "import numpy as np\n",
        "# drop duplicate translations\n",
        "df_pp = df.drop_duplicates()\n",
        "\n",
        "#drop empty lines (alp)\n",
        "df_pp['source_sentence'].replace('', np.nan, inplace=True)\n",
        "df_pp['target_sentence'].replace('', np.nan, inplace=True)\n",
        "df_pp.dropna(subset=['source_sentence'], inplace=True)\n",
        "df_pp.dropna(subset=['target_sentence'], inplace=True)\n",
        "\n",
        "# drop conflicting translations\n",
        "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/series.py:4582: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  method=method,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD9Sr3Jlx2y4",
        "outputId": "fb433d7d-6e1e-4353-f8ed-439e627f3bca"
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
        "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
        "\n",
        "# Julia: test sets are already generated\n",
        "dev = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "\n",
        "# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
        "! head train.*\n",
        "! head dev.*\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> train.en <==\n",
            "His desire is that you attain to “ the real life ” — everlasting life on a paradise earth .\n",
            "BE POSITIVE : “ All the days of the afflicted one are bad , but the one with a cheerful heart has a continual feast . ”\n",
            "Peter reached the gate and was refused entrance .\n",
            "Sudden sharp sounds reverberated like rifle shots .\n",
            "The record tells us : “ He opened the scroll and found the place where it was written : ‘ Jehovah ’ s spirit is upon me , because he anointed me to declare good news to the poor , he sent me forth to preach a release to the captives and a recovery of sight to the blind , to send the crushed ones away with a release . ’\n",
            "( Num . 25 : 1 , 2 ) Jehovah afflicted the wrongdoers with a death-dealing scourge .\n",
            "The Bible says that God “ heals the brokenhearted ; he binds up their wounds . ”\n",
            "Cristina , Italy : “ When a problem seems beyond my ability to solve , I pray to Jehovah , leaving it in his hands .\n",
            "▪ How did Jesus ’ followers view the secular authorities ? — Romans 13 : 7 .\n",
            "Notice how obedience was inseparably linked to receiving God ’ s approval .\n",
            "\n",
            "==> train.sw <==\n",
            "Anataka upate “ uhai ulio halisi , ” yaani , uhai wa milele katika dunia paradiso .\n",
            "UWE NA MTAZAMO UNAOFAA : “ Siku zote za mwenye kuteseka ni mbaya , lakini mtu aliye mchangamfu moyoni huwa na karamu sikuzote . ”\n",
            "Alipofika langoni , Petro alikatazwa kuingia ndani .\n",
            "Sauti kali za ghafula zilivuma kama mlio wa bunduki .\n",
            "Rekodi yatuambia hivi : “ Akakifungua chuo , akatafuta mahali palipoandikwa , Roho wa Bwana yu [ roho ya Yehova i , NW ] juu yangu , kwa maana amenitia mafuta kuwahubiri maskini habari njema . Amenituma kuwatangazia wafungwa kufunguliwa kwao , na vipofu kupata kuona tena , kuwaacha huru waliosetwa . ”\n",
            "( Hes . 25 : 1 , 2 ) Yehova aliwaadhibu waovu kwa tauni iliyosababisha kifo .\n",
            "Biblia inasema kwamba Mungu “ anawaponya wenye mioyo iliyovunjika , naye anafunga sehemu zao zenye maumivu . ”\n",
            "Cristina , Italia : “ Ninapoona kwamba siwezi kutatua tatizo fulani , mimi husali kwa Yehova , na kuliacha mikononi mwake .\n",
            "▪ Wafuasi wa Yesu walikuwa na maoni gani kuhusu wenye mamlaka serikalini ? — Waroma 13 : 7 .\n",
            "Ona jinsi utii ulivyofungamanishwa sikuzote na kupokea kibali cha Mungu .\n",
            "==> dev.en <==\n",
            "But can these tunics be used in all Catholic churches ?\n",
            "I WAS born into a “ Christian ” household in a Central African country , and I grew up with a love for God .\n",
            "Every day local newspapers carry reports of elephant damage . ”\n",
            "Jeanine brings up another problem : “ It can be a real challenge to reject the tendency to make your son the head of the household so as to make up for the absence of a husband .\n",
            "The Bible warns : “ He that is walking with wise persons will become wise , but he that is having dealings with the stupid ones will fare badly . ” — Proverbs 13 : 20 .\n",
            "Front\n",
            "Now , over a half century later , it is apparent that Bernal was correct .\n",
            "During that year , 1542 , one Indian tribe after another suffered similar shocks as those European explorers , flashing crossbows and harquebuses , pushed ever deeper into South America ’ s tropical forest .\n",
            "I decided to try sharing in this feature of service that required devoting 75 hours a month to the ministry .\n",
            "Indeed , what the Christian apostle Paul wrote over 1,900 years ago has even more force today : “ All creation keeps on groaning together and being in pain together until now . ” — Romans 8 : 22 .\n",
            "\n",
            "==> dev.sw <==\n",
            "Lakini je , hizi kanzu fupi zaweza kuvaliwa katika makanisa yote ya Kikatoliki ?\n",
            "MIMI nilizaliwa katika nyumba “ ya Kikristo ” katika nchi moja ya Afrika ya Kati , na nilikua nikiwa nampenda Mungu .\n",
            "Kila siku magazeti ya habari ya mahali hapo huwa na ripoti za uharibifu wa tembo . ”\n",
            "Jeanine ataja tatizo jingine : “ Inaweza kuwa vigumu sana kukinza mwelekeo wa kutaka mwanao atende kama kichwa cha nyumba yako ili kujazia pengo la kutokuwa na mume .\n",
            "Biblia inaonya hivi : “ Anayetembea na watu wenye hekima atakuwa na hekima , lakini anayeshirikiana na wajinga atapatwa na mabaya . ” — Methali 13 : 20 .\n",
            "Mbele\n",
            "Sasa , zaidi ya nusu karne baadaye , ni wazi kwamba Bernal alisema kweli .\n",
            "Katika mwaka huo , 1542 , kabila moja la Kihindi baada ya jingine lilipatwa na mishtuko kama hiyo huku wavumbuzi wa Ulaya , wenye kupunga pinde na bunduki aina ya harquebuses , wakipenya ndani zaidi ya msitu wa kitropiki wa Amerika Kusini .\n",
            "Nikaamua kujaribu kushiriki katika sehemu hiyo ya utumishi iliyotaka mtu kutoa muda wa saa 75 kwa mwezi katika huduma .\n",
            "Kwa kweli , yale ambayo mtume Mkristo Paulo aliandika zaidi ya miaka 1,900 iliyopita yana nguvu hata zaidi leo : “ Viumbe vyote vinaugua pamoja , navyo vina utungu pamoja hata sasa . ” — Warumi 8 : 22 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EsTgrcTyiGQ",
        "outputId": "70edfc2b-b5d9-4880-f3c3-593db01359a2"
      },
      "source": [
        "#Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "# !rm -rf joeynmt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 3224, done.\u001b[K\n",
            "remote: Counting objects: 100% (273/273), done.\u001b[K\n",
            "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
            "remote: Total 3224 (delta 157), reused 206 (delta 134), pack-reused 2951\u001b[K\n",
            "Receiving objects: 100% (3224/3224), 8.17 MiB | 15.43 MiB/s, done.\n",
            "Resolving deltas: 100% (2186/2186), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4fVjOnjHbQA7",
        "outputId": "07e3562e-9c8b-4e43-c7d7-039410d1b579"
      },
      "source": [
        "!pip install joeynmt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting joeynmt\n",
            "  Downloading joeynmt-1.3-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting pylint\n",
            "  Downloading pylint-2.11.1-py3-none-any.whl (392 kB)\n",
            "\u001b[K     |████████████████████████████████| 392 kB 32.7 MB/s \n",
            "\u001b[?25hCollecting numpy==1.20.1\n",
            "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 106 kB/s \n",
            "\u001b[?25hCollecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 11 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (2.6.0)\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 12.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt) (3.2.2)\n",
            "Collecting sacrebleu>=1.3.6\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (57.4.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.11.2)\n",
            "Collecting six==1.12\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt) (3.7.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt) (4.62.3)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt) (2019.12.20)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt) (0.8.9)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.41.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.3.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.4.7)\n",
            "Collecting isort<6,>=4.2.5\n",
            "  Downloading isort-5.9.3-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 48.7 MB/s \n",
            "\u001b[?25hCollecting astroid<2.9,>=2.8.0\n",
            "  Downloading astroid-2.8.3-py3-none-any.whl (246 kB)\n",
            "\u001b[K     |████████████████████████████████| 246 kB 45.2 MB/s \n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting mccabe<0.7,>=0.6\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt) (0.10.2)\n",
            "Collecting platformdirs>=2.2.0\n",
            "  Downloading platformdirs-2.4.0-py3-none-any.whl (14 kB)\n",
            "Collecting typed-ast<1.5,>=1.4.0\n",
            "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 44.1 MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
            "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt) (2018.9)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68437 sha256=2bd7b855ab55112f1e2b10ecb6dc4109422a3e550450d790bb8487a901373d01\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, six, wrapt, typed-ast, numpy, lazy-object-proxy, torch, portalocker, platformdirs, mccabe, isort, colorama, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.7.4.3\n",
            "    Uninstalling typing-extensions-3.7.4.3:\n",
            "      Successfully uninstalled typing-extensions-3.7.4.3\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu111\n",
            "    Uninstalling torch-1.9.0+cu111:\n",
            "      Successfully uninstalled torch-1.9.0+cu111\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.8.0 which is incompatible.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
            "tensorflow 2.6.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "tensorflow 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "tensorflow 2.6.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed astroid-2.8.3 colorama-0.4.4 isort-5.9.3 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 platformdirs-2.4.0 portalocker-2.3.2 pylint-2.11.1 pyyaml-6.0 sacrebleu-2.0.0 six-1.12.0 subword-nmt-0.3.7 torch-1.8.0 torchtext-0.9.0 typed-ast-1.4.3 typing-extensions-3.10.0.2 wrapt-1.11.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six",
                  "typing_extensions",
                  "wrapt"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzlM-LX24wM_"
      },
      "source": [
        "! cd joeynmt;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv-esfZBywaK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oL8GUjfzQ2f"
      },
      "source": [
        "\n",
        "#### Preprocessing the Data into Subword BPE Tokens\n",
        "One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization (Sennrich, 2015) .\n",
        "\n",
        "It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages (Sennrich, 2019) (Martinus, 2019)\n",
        "\n",
        "Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by (Sennrich, 2019). You do not need to change anything. Simply running the below will be suitable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4NYuJvlAq4i"
      },
      "source": [
        "# path='/content/drive/MyDrive/colab/MT_TRANSALATOR/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL7YuXltDnOE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM9jpOnZzUEo"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdLv8dMfzZUZ"
      },
      "source": [
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuVEmIiJzxIv",
        "outputId": "f7869c3a-4ae7-45c3-f5ff-2468839fe1a1"
      },
      "source": [
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.sw     test.sw\t   train.en\n",
            "dev.bpe.en\tdev.sw\t     test.en\t     train.bpe.en  train.sw\n",
            "dev.bpe.sw\ttest.bpe.en  test.en-any.en  train.bpe.sw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Xp7Dy-Pz2Jg",
        "outputId": "0716657a-5232-45a8-92b3-c4b31e1a9e75"
      },
      "source": [
        "#Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.*  \"$gdrive_path\"\n",
        "! cp dev.*  \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `\"'\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXMUfgxnN0xX",
        "outputId": "7e4209f7-0e70-4290-f91f-938b551c090c"
      },
      "source": [
        "!ls joeynmt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "benchmarks.md\t    data\t     joeynmt\t     README.md\t       setup.py\n",
            "CODE_OF_CONDUCT.md  docs\t     joey-small.png  requirements.txt  test\n",
            "configs\t\t    joey_demo.ipynb  LICENSE\t     scripts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmeHvQ1iz-JR"
      },
      "source": [
        "# Create that vocab using build_vocab\n",
        "# ! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URfByZ3sz-OX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd05UT83z2OP",
        "outputId": "be099a1e-b023-4b97-a528-5933e48d7ea4"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.sw     test.en-any.en.1  train.bpe.sw\n",
            "dev.bpe.en\tdev.sw\t     test.en\t     test.sw\t       train.en\n",
            "dev.bpe.sw\ttest.bpe.en  test.en-any.en  train.bpe.en      train.sw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyHUJcj27b-5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl2wKJct7j5w"
      },
      "source": [
        "\n",
        "####Creating the JoeyNMT Config\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "We used Transformer architecture\n",
        "We set our dropout to reasonably high: 0.3 (recommended in (Sennrich, 2019))\n",
        "Things worth playing with:\n",
        "\n",
        "The batch size (also recommended to change for low-resourced languages)\n",
        "The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "The decoder options (beam_size, alpha)\n",
        "Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM4TjUUZ7mUY"
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "# gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_filtered_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"/content/joeynmt/data/{name}/train.bpe\"\n",
        "    dev:   \"/content/joeynmt/data/{name}/dev.bpe\"\n",
        "    test:  \"/content/joeynmt/data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"/content/joeynmt/data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"/content/joeynmt/data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"/content/joeynmt/models/{name}_filtered_transformer\"\n",
        "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=\"joeynmt\", source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_filtered_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlQSwFttDqPu",
        "outputId": "dd65cdb0-8438-49f2-caa7-b1481649438f"
      },
      "source": [
        "!readlink -f train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWyThMc3EouW",
        "outputId": "d095d7be-1b59-4bd4-873e-b32453c03f27"
      },
      "source": [
        "!ls joeynmt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "benchmarks.md\t    data\t     joeynmt\t     README.md\t       setup.py\n",
            "CODE_OF_CONDUCT.md  docs\t     joey-small.png  requirements.txt  test\n",
            "configs\t\t    joey_demo.ipynb  LICENSE\t     scripts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCgELS7C73kF",
        "outputId": "b726304f-ca9f-4c74-d4b9-c8c1587a6dfc"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python -m joeynmt train /content/joeynmt/configs/transformer_filtered_$src$tgt.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-10-22 07:41:05,503 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-10-22 07:41:05,581 - INFO - joeynmt.data - Loading training data...\n",
            "2021-10-22 07:41:45,126 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-10-22 07:41:45,156 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-10-22 07:41:45,244 - INFO - joeynmt.data - Loading test data...\n",
            "2021-10-22 07:41:45,290 - INFO - joeynmt.data - Data loaded.\n",
            "2021-10-22 07:41:45,291 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-10-22 07:41:45,817 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-10-22 07:41:50,509 - INFO - joeynmt.training - Total params: 11197184\n",
            "2021-10-22 07:41:50,514 - WARNING - joeynmt.training - `keep_last_ckpts` option is outdated. Please use `keep_best_ckpts`, instead.\n",
            "2021-10-22 07:41:58,215 - INFO - joeynmt.helpers - cfg.name                           : ensw_filtered_transformer\n",
            "2021-10-22 07:41:58,216 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
            "2021-10-22 07:41:58,216 - INFO - joeynmt.helpers - cfg.data.trg                       : sw\n",
            "2021-10-22 07:41:58,216 - INFO - joeynmt.helpers - cfg.data.train                     : /content/joeynmt/data/ensw/train.bpe\n",
            "2021-10-22 07:41:58,216 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/joeynmt/data/ensw/dev.bpe\n",
            "2021-10-22 07:41:58,216 - INFO - joeynmt.helpers - cfg.data.test                      : /content/joeynmt/data/ensw/test.bpe\n",
            "2021-10-22 07:41:58,216 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-10-22 07:41:58,217 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-10-22 07:41:58,217 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-10-22 07:41:58,217 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/joeynmt/data/ensw/vocab.txt\n",
            "2021-10-22 07:41:58,217 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/joeynmt/data/ensw/vocab.txt\n",
            "2021-10-22 07:41:58,217 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-10-22 07:41:58,217 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-10-22 07:41:58,217 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-10-22 07:41:58,218 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-10-22 07:41:58,218 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-10-22 07:41:58,218 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-10-22 07:41:58,218 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-10-22 07:41:58,218 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-10-22 07:41:58,218 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-10-22 07:41:58,218 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-10-22 07:41:58,218 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-10-22 07:41:58,219 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-10-22 07:41:58,219 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-10-22 07:41:58,219 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-10-22 07:41:58,219 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-10-22 07:41:58,219 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-10-22 07:41:58,219 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-10-22 07:41:58,219 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-10-22 07:41:58,220 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-10-22 07:41:58,220 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-10-22 07:41:58,220 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-10-22 07:41:58,220 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-10-22 07:41:58,220 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-10-22 07:41:58,220 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000\n",
            "2021-10-22 07:41:58,220 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-10-22 07:41:58,220 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-10-22 07:41:58,221 - INFO - joeynmt.helpers - cfg.training.model_dir             : /content/joeynmt/models/ensw_filtered_transformer\n",
            "2021-10-22 07:41:58,221 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
            "2021-10-22 07:41:58,221 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-10-22 07:41:58,221 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-10-22 07:41:58,221 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-10-22 07:41:58,221 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-10-22 07:41:58,221 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-10-22 07:41:58,221 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-10-22 07:41:58,222 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-10-22 07:41:58,222 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-10-22 07:41:58,222 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-10-22 07:41:58,222 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-10-22 07:41:58,222 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-10-22 07:41:58,222 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-10-22 07:41:58,222 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-10-22 07:41:58,222 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-10-22 07:41:58,223 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-10-22 07:41:58,223 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-10-22 07:41:58,223 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-10-22 07:41:58,223 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-10-22 07:41:58,223 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-10-22 07:41:58,223 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-10-22 07:41:58,223 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-10-22 07:41:58,223 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-10-22 07:41:58,224 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-10-22 07:41:58,224 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-10-22 07:41:58,224 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-10-22 07:41:58,224 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-10-22 07:41:58,224 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-10-22 07:41:58,224 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-10-22 07:41:58,224 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-10-22 07:41:58,224 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-10-22 07:41:58,225 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 563850,\n",
            "\tvalid 1000,\n",
            "\ttest 2721\n",
            "2021-10-22 07:41:58,225 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] H@@ i@@ s d@@ e@@ s@@ i@@ r@@ e i@@ s t@@ h@@ a@@ t y@@ o@@ u a@@ t@@ t@@ a@@ i@@ n t@@ o “ t@@ h@@ e r@@ e@@ a@@ l l@@ i@@ f@@ e ” — e@@ v@@ e@@ r@@ l@@ a@@ s@@ t@@ i@@ n@@ g l@@ i@@ f@@ e o@@ n a p@@ a@@ r@@ a@@ d@@ i@@ s@@ e e@@ a@@ r@@ t@@ h .\n",
            "\t[TRG] A@@ n@@ a@@ t@@ a@@ k@@ a u@@ p@@ a@@ t@@ e “ u@@ h@@ a@@ i u@@ l@@ i@@ o h@@ a@@ l@@ i@@ s@@ i , ” y@@ a@@ a@@ n@@ i , u@@ h@@ a@@ i w@@ a m@@ i@@ l@@ e@@ l@@ e k@@ a@@ t@@ i@@ k@@ a d@@ u@@ n@@ i@@ a p@@ a@@ r@@ a@@ d@@ i@@ s@@ o .\n",
            "2021-10-22 07:41:58,225 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) a@@ (5) i@@ (6) e@@ (7) n@@ (8) a (9) t@@\n",
            "2021-10-22 07:41:58,225 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) a@@ (5) i@@ (6) e@@ (7) n@@ (8) a (9) t@@\n",
            "2021-10-22 07:41:58,225 - INFO - joeynmt.helpers - Number of Src words (types): 535\n",
            "2021-10-22 07:41:58,226 - INFO - joeynmt.helpers - Number of Trg words (types): 535\n",
            "2021-10-22 07:41:58,226 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=535),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=535))\n",
            "2021-10-22 07:41:58,227 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-10-22 07:41:58,227 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-10-22 07:42:35,565 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     2.398942, Tokens per Sec:     6346, Lr: 0.000300\n",
            "2021-10-22 07:43:11,369 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     2.292622, Tokens per Sec:     6638, Lr: 0.000300\n",
            "2021-10-22 07:43:47,087 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     2.229941, Tokens per Sec:     6657, Lr: 0.000300\n",
            "2021-10-22 07:44:22,836 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     2.201759, Tokens per Sec:     6674, Lr: 0.000300\n",
            "2021-10-22 07:44:58,415 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     2.160370, Tokens per Sec:     6623, Lr: 0.000300\n",
            "2021-10-22 07:45:34,044 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     2.195939, Tokens per Sec:     6626, Lr: 0.000300\n",
            "2021-10-22 07:46:09,550 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     2.100520, Tokens per Sec:     6674, Lr: 0.000300\n",
            "2021-10-22 07:46:45,146 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     2.121997, Tokens per Sec:     6591, Lr: 0.000300\n",
            "2021-10-22 07:47:20,658 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     2.033814, Tokens per Sec:     6646, Lr: 0.000300\n",
            "2021-10-22 07:47:56,132 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     2.071957, Tokens per Sec:     6681, Lr: 0.000300\n",
            "2021-10-22 07:50:04,699 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-10-22 07:50:04,700 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-10-22 07:50:04,700 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2021-10-22 07:50:04,714 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-10-22 07:50:05,246 - INFO - joeynmt.training - Example #0\n",
            "2021-10-22 07:50:05,246 - INFO - joeynmt.training - \tSource:     But can these tunics be used in all Catholic churches ?\n",
            "2021-10-22 07:50:05,246 - INFO - joeynmt.training - \tReference:  Lakini je , hizi kanzu fupi zaweza kuvaliwa katika makanisa yote ya Kikatoliki ?\n",
            "2021-10-22 07:50:05,246 - INFO - joeynmt.training - \tHypothesis: Kwa nini kutatika kutatika kutani ya Mangu ?\n",
            "2021-10-22 07:50:05,247 - INFO - joeynmt.training - Example #1\n",
            "2021-10-22 07:50:05,247 - INFO - joeynmt.training - \tSource:     I WAS born into a “ Christian ” household in a Central African country , and I grew up with a love for God .\n",
            "2021-10-22 07:50:05,247 - INFO - joeynmt.training - \tReference:  MIMI nilizaliwa katika nyumba “ ya Kikristo ” katika nchi moja ya Afrika ya Kati , na nilikua nikiwa nampenda Mungu .\n",
            "2021-10-22 07:50:05,247 - INFO - joeynmt.training - \tHypothesis: “ Nini kuwa Mangu na Mangu ya Mungu , “ Mangu ananananananangu , ” ana kutana kutana kutana . ”\n",
            "2021-10-22 07:50:05,247 - INFO - joeynmt.training - Example #2\n",
            "2021-10-22 07:50:05,248 - INFO - joeynmt.training - \tSource:     Every day local newspapers carry reports of elephant damage . ”\n",
            "2021-10-22 07:50:05,248 - INFO - joeynmt.training - \tReference:  Kila siku magazeti ya habari ya mahali hapo huwa na ripoti za uharibifu wa tembo . ”\n",
            "2021-10-22 07:50:05,248 - INFO - joeynmt.training - \tHypothesis: Kwa kutamba kutana mani ya mangani ya mani yanyenye . ”\n",
            "2021-10-22 07:50:05,248 - INFO - joeynmt.training - Example #3\n",
            "2021-10-22 07:50:05,248 - INFO - joeynmt.training - \tSource:     Jeanine brings up another problem : “ It can be a real challenge to reject the tendency to make your son the head of the household so as to make up for the absence of a husband .\n",
            "2021-10-22 07:50:05,248 - INFO - joeynmt.training - \tReference:  Jeanine ataja tatizo jingine : “ Inaweza kuwa vigumu sana kukinza mwelekeo wa kutaka mwanao atende kama kichwa cha nyumba yako ili kujazia pengo la kutokuwa na mume .\n",
            "2021-10-22 07:50:05,248 - INFO - joeynmt.training - \tHypothesis: Yehova alikuwa kuwa kuwa kutamba kwa kutamba kutana kutamba ya kutana kutamba ya kutangani .\n",
            "2021-10-22 07:50:05,249 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     1000: bleu:   0.32, loss: 176431.8125, ppl:   7.8186, duration: 129.1158s\n",
            "2021-10-22 07:50:40,849 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.002202, Tokens per Sec:     6688, Lr: 0.000300\n",
            "2021-10-22 07:51:16,425 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     1.973887, Tokens per Sec:     6626, Lr: 0.000300\n",
            "2021-10-22 07:51:52,106 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     2.010974, Tokens per Sec:     6669, Lr: 0.000300\n",
            "2021-10-22 07:52:27,681 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     1.935511, Tokens per Sec:     6573, Lr: 0.000300\n",
            "2021-10-22 07:53:03,279 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     1.889287, Tokens per Sec:     6639, Lr: 0.000300\n",
            "2021-10-22 07:53:38,909 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     1.882241, Tokens per Sec:     6579, Lr: 0.000300\n",
            "2021-10-22 07:54:14,553 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     1.885085, Tokens per Sec:     6660, Lr: 0.000300\n",
            "2021-10-22 07:54:50,147 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     1.913878, Tokens per Sec:     6566, Lr: 0.000300\n",
            "2021-10-22 07:55:25,817 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     1.860279, Tokens per Sec:     6686, Lr: 0.000300\n",
            "2021-10-22 07:56:01,497 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     1.818759, Tokens per Sec:     6783, Lr: 0.000300\n",
            "2021-10-22 07:58:16,163 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-10-22 07:58:16,164 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-10-22 07:58:16,164 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2021-10-22 07:58:16,171 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-10-22 07:58:16,689 - INFO - joeynmt.training - Example #0\n",
            "2021-10-22 07:58:16,689 - INFO - joeynmt.training - \tSource:     But can these tunics be used in all Catholic churches ?\n",
            "2021-10-22 07:58:16,689 - INFO - joeynmt.training - \tReference:  Lakini je , hizi kanzu fupi zaweza kuvaliwa katika makanisa yote ya Kikatoliki ?\n",
            "2021-10-22 07:58:16,690 - INFO - joeynmt.training - \tHypothesis: Lakini kwamba kutoka kutoka kutoka kutoka kutoka kutoka ?\n",
            "2021-10-22 07:58:16,690 - INFO - joeynmt.training - Example #1\n",
            "2021-10-22 07:58:16,690 - INFO - joeynmt.training - \tSource:     I WAS born into a “ Christian ” household in a Central African country , and I grew up with a love for God .\n",
            "2021-10-22 07:58:16,690 - INFO - joeynmt.training - \tReference:  MIMI nilizaliwa katika nyumba “ ya Kikristo ” katika nchi moja ya Afrika ya Kati , na nilikua nikiwa nampenda Mungu .\n",
            "2021-10-22 07:58:16,690 - INFO - joeynmt.training - \tHypothesis: Nilikuwa nilikuwa “ Nikuwa na mambo ya Mungu , ” alikuwa na kutoka matika mambo ya Mungu .\n",
            "2021-10-22 07:58:16,690 - INFO - joeynmt.training - Example #2\n",
            "2021-10-22 07:58:16,691 - INFO - joeynmt.training - \tSource:     Every day local newspapers carry reports of elephant damage . ”\n",
            "2021-10-22 07:58:16,691 - INFO - joeynmt.training - \tReference:  Kila siku magazeti ya habari ya mahali hapo huwa na ripoti za uharibifu wa tembo . ”\n",
            "2021-10-22 07:58:16,691 - INFO - joeynmt.training - \tHypothesis: Kwa mambo ya kutoka mambo ya kitika matika mani ya kutoka matika matika . ”\n",
            "2021-10-22 07:58:16,691 - INFO - joeynmt.training - Example #3\n",
            "2021-10-22 07:58:16,691 - INFO - joeynmt.training - \tSource:     Jeanine brings up another problem : “ It can be a real challenge to reject the tendency to make your son the head of the household so as to make up for the absence of a husband .\n",
            "2021-10-22 07:58:16,691 - INFO - joeynmt.training - \tReference:  Jeanine ataja tatizo jingine : “ Inaweza kuwa vigumu sana kukinza mwelekeo wa kutaka mwanao atende kama kichwa cha nyumba yako ili kujazia pengo la kutokuwa na mume .\n",
            "2021-10-22 07:58:16,691 - INFO - joeynmt.training - \tHypothesis: Yehova alikuwa na mambo ya Yehova alikuwa na mambo ya kuwa na mambo ya kutumia make ya kutoka matika .\n",
            "2021-10-22 07:58:16,692 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     2000: bleu:   1.10, loss: 160149.6875, ppl:   6.4671, duration: 135.1942s\n",
            "2021-10-22 07:58:52,321 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     1.824459, Tokens per Sec:     6647, Lr: 0.000300\n",
            "2021-10-22 07:59:27,935 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     1.773832, Tokens per Sec:     6689, Lr: 0.000300\n",
            "2021-10-22 08:00:03,423 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     1.714975, Tokens per Sec:     6654, Lr: 0.000300\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
            "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 846, in train\n",
            "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 447, in train_and_validate\n",
            "    batch_loss += self._train_step(batch)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 539, in _train_step\n",
            "    batch_loss, _, _, _ = self.model(return_type=\"loss\", **vars(batch))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/model.py\", line 90, in forward\n",
            "    batch_loss = self.loss_function(log_probs, kwargs[\"trg\"])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/loss.py\", line 69, in forward\n",
            "    vocab_size=log_probs.size(-1))\n",
            "  File \"/content/joeynmt/joeynmt/loss.py\", line 47, in _smooth_targets\n",
            "    as_tuple=False)\n",
            "RuntimeError: CUDA error: unknown error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7GzwKNd7uEK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}